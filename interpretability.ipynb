{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653a2512-41af-464d-bb5c-2baec675c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e101eb-6d0c-4ddd-8981-97a7010c406b",
   "metadata": {},
   "source": [
    "# Problem 1: Feature Importance in Iris Classification\n",
    "We return to the iris dataset from last week. Load it into the notebook and make a pair plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac04bcd-0afd-44e2-8bb6-81432f45e3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b7dd80-b91d-4a8f-aeb7-be4b1baa8f18",
   "metadata": {},
   "source": [
    "This time, let's classify it using a random forest. Initialize and train the random forest classifer using cross validation. Repeat the above plot with the predicted classifications and calculate the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c783edc5-f561-43c9-9864-9f075e150716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete\n",
    "# complete\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca5fa05-ebc6-419f-896d-780eb634d99c",
   "metadata": {},
   "source": [
    "Calculate the mean decrease in impurity and the permutation importance for each feature and plot them (*hint:* `plt.barh`). Which feature(s) seem to be the most useful for classification? Does this agree with your intuition from the plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35c5123-ef7b-4b16-ad5b-5f71c95a6026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d89ee-7c9e-41e2-ba61-74ad1d84aaec",
   "metadata": {},
   "source": [
    "*Complete*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af357db-bb91-493d-b213-cc431707ae5b",
   "metadata": {},
   "source": [
    "However, note from the pair plot that the petal length and width are strongly correlated. Pick one of these features and remove it from the data set. Then recalculate the accuracy and the feature importance using only 3 features. How do these change, and do the changes make sense given what you know about the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4076e4ae-363c-4e56-9333-00c89db1102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete\n",
    "# complete\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b87be1-e6ac-4a54-8a22-edfd8444f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete\n",
    "# complete\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711b010b-ed5d-4276-977a-ef9772050207",
   "metadata": {},
   "source": [
    "*Complete*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf11e93e-d824-40df-a74c-82980ad88ce0",
   "metadata": {},
   "source": [
    "# Problem 2: Efficient Classification of a Higher Dimensional Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd26cf-159a-46ff-af28-f3f32f01f9c5",
   "metadata": {},
   "source": [
    "As much time as we've spent on the iris data set, it is much simpler to understand and easier to classify than almost any data set you will encounter in the real world. Let's instead work with the wine data set, which includes 13 chemical and physical measurements of samples of wine from three different (anonymous) vineyards in Italy. Start by reading it in and plotting it. What do you notice about this data set? Are there any single features that divide the three classes cleanly? Will the ranges of the features cause any problems for our classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc5e22-bcbf-4117-b93d-035a901dd5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f152e6-d94d-4687-b0c7-39104b65ec6e",
   "metadata": {},
   "source": [
    "*Complete*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3682fdfb-18cc-4d6f-adc8-bd59a238a911",
   "metadata": {},
   "source": [
    "In the end, we would like to understand how our classifier works, so let's try to reduce the number of features to something more manageable. Apply a dimensionality reduction technique and decide how many features we can make do with. (This is intentionally very open-ended to make you think about it!) Comment on what you're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e49914-7498-461a-9bc4-02d67b6a48e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete\n",
    "# complete\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe9ab95-84d3-4e73-878e-337d4dc6a057",
   "metadata": {},
   "source": [
    "Using your newly engineered features, train a support vector machine with cross validation and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b73360-fb89-45f0-9d15-70ed58036540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "\n",
    "# complete\n",
    "# complete\n",
    "\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317c1205-fa9a-4870-be4b-830976d44228",
   "metadata": {},
   "source": [
    "Lastly, because this is the interpretability notebook, we want to gain some understanding of how this classification procedure actually worked. Make a graph of how your new features relate to the original input features. Based on the graph, which of the original features are most influential in the final classification? Does your answer differ for different classes? How much of the total sample variance does each of the new features explain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92fc5e3-b18c-447d-ab00-f4e2bbbad8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d2013-4e51-4c7c-8b01-d042a87def1a",
   "metadata": {},
   "source": [
    "*Complete*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab708021-4c96-4a7d-999f-52f2a03dd123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
